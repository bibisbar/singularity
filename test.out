output dir >> /home/wiss/zhang/Jinhe/singularity/neg/ret_anet/anet_anet_neg
rdzv_endpoint: zypern:22347
[32m2023-10-20T13:47:05 | loopitr: [0mLogging to: /home/wiss/zhang/Jinhe/singularity/neg/ret_anet/anet_anet_neg/train.log

[32m2023-10-20T13:47:13 | __main__: [0mconfig: 
{'data_root': '/home/wiss/zhang/nfs/Anet_sing', 'anno_root_downstream': '/home/wiss/zhang/Jinhe/singularity/Data/anetqa', 'train_type': 'anet_neg.json', 'train_file': ['${anno_root_downstream}/${train_type}', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'test_types': ['temporal_contact_swap', 'temporal_contact_swap_mani', 'temporal_action_swap', 'temporal_action_swap_mani', 'neighborhood_same_entity', 'neighborhood_same_entity_mani', 'neighborhood_diff_entity', 'neighborhood_diff_entity_mani', 'counter_spatial', 'counter_spatial_mani', 'counter_contact', 'counter_contact_mani', 'counter_action', 'counter_action_mani', 'counter_attribute', 'counter_attribute_mani'], 'test_file': {'temporal_contact_swap': ['${anno_root_downstream}/anet_ret_temporal_contact_swap.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'temporal_contact_swap_mani': ['${anno_root_downstream}/anet_ret_temporal_contact_swap_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'temporal_action_swap': ['${anno_root_downstream}/anet_ret_temporal_action_swap.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'temporal_action_swap_mani': ['${anno_root_downstream}/anet_ret_temporal_action_swap_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_same_entity': ['${anno_root_downstream}/anet_ret_neighborhood_same_entity.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_same_entity_mani': ['${anno_root_downstream}/anet_ret_neighborhood_same_entity_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_diff_entity': ['${anno_root_downstream}/anet_ret_neighborhood_diff_entity.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_diff_entity_mani': ['${anno_root_downstream}/anet_ret_neighborhood_diff_entity_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_spatial': ['${anno_root_downstream}/anet_ret_counter_spatial.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_spatial_mani': ['${anno_root_downstream}/anet_ret_counter_spatial_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_contact': ['${anno_root_downstream}/anet_ret_counter_contact.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_contact_mani': ['${anno_root_downstream}/anet_ret_counter_contact_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_action': ['${anno_root_downstream}/anet_ret_counter_action.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_action_mani': ['${anno_root_downstream}/anet_ret_counter_action_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_attribute': ['${anno_root_downstream}/anet_ret_counter_attribute.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_attribute_mani': ['${anno_root_downstream}/anet_ret_counter_attribute_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video']}, 'stop_key': 'val1/', 'is_paragraph_retrieval': True, 'text_encoder': 'bert-base-uncased', 'bert_config': 'configs/config_bert.json', 'vit_type': 'beit', 'vit_zoo': {'beit': 'microsoft/beit-base-patch16-224-pt22k-ft22k'}, 'vit_name_or_pretrained_path': '${vit_zoo[${vit_type}]}', 'temporal_vision_encoder': {'enable': True, 'num_layers': 2, 'update_pooler_embed': False}, 'add_temporal_embed': True, 'image_res': 224, 'embed_dim': 256, 'video_input': {'num_frames': 4, 'reader': 'decord', 'sample_type': 'rand', 'num_frames_test': 4, 'sample_type_test': 'middle'}, 'max_txt_l': 60, 'batch_size': {'image': 160, 'video': 4}, 'batch_size_test': {'image': 128, 'video': 32}, 'k_test': 128, 'temp': 0.01, 'loss_weight': {'itc': 1.0, 'itm': 1.0}, 'itm_hard_neg': True, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 30, 'min_lr_multi': 0.1, 'warmup_epochs': 0, 'epoch': 30}, 'output_dir': '/home/wiss/zhang/Jinhe/singularity/neg/ret_anet/anet_anet_neg', 'resume': False, 'pretrained_path': '/home/wiss/zhang/nfs/anetqa_train_qa_full/ckpt_best.pth', 'evaluate': False, 'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'eval_offload': True, 'device': 'cuda', 'seed': 42, 'log_freq': 100, 'dist_url': 'env://', 'distributed': True, 'fp16': True, 'debug': False, 'num_workers': 24, 'wandb': {'enable': True, 'entity': 'gengyuanzhang', 'project': 'sb_ret_anet'}, 'save_path': '/home/wiss/zhang/nfs/video_prober/singularity/anetqa/', '22347': None, 'rank': 0, 'world_size': 1, 'gpu': 0, 'dist_backend': 'nccl'}
[32m2023-10-20T13:47:13 | __main__: [0mtrain_file: ['${anno_root_downstream}/${train_type}', '/home/wiss/zhang/nfs/Anet_sing', 'video']
[32m2023-10-20T13:47:13 | tasks.pretrain: [0mCreating dataset for ret
[5m[31mWARNING[0m [32m2023-10-20T13:47:14 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  builtin_warn(*args, **kwargs)

[5m[31mWARNING[0m [32m2023-10-20T13:47:14 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  builtin_warn(*args, **kwargs)

[32m2023-10-20T13:47:14 | tasks.shared_utils: [0mCreating model
[32m2023-10-20T13:47:17 | models.model_retrieval_base: [0mLoading vit pre-trained weights from huggingface microsoft/beit-base-patch16-224-pt22k-ft22k.
[5m[31mWARNING[0m [32m2023-10-20T13:47:22 | py.warnings: [0m/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[5m[31mWARNING[0m [32m2023-10-20T13:47:22 | py.warnings: [0m/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[32m2023-10-20T13:47:23 | models.model_retrieval_base: [0mInit new model with new image size 224, and load weights.
[32m2023-10-20T13:47:25 | models.model_retrieval_base: [0m_IncompatibleKeys(missing_keys=['encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.11.attention.attention.relative_position_bias.relative_position_index'], unexpected_keys=[])
[32m2023-10-20T13:47:25 | models.model_retrieval_base: [0mBuild text_encoder bert-base-uncased
[32m2023-10-20T13:47:31 | models.model_retrieval_base: [0mBuild text_encoder bert-base-uncased, done!
[32m2023-10-20T13:47:31 | models.model_retrieval_base: [0mBuild temporal_vision_encoder (#layer=2), randomly initialised.
[32m2023-10-20T13:47:31 | models.model_retrieval_base: [0mBuild temporal_vision_encoder, done!
[32m2023-10-20T13:47:31 | utils.optimizer: [0moptimizer -- lr=1e-05 wd=0.02 len(p)=190
[32m2023-10-20T13:47:31 | utils.optimizer: [0moptimizer -- lr=1e-05 wd=0 len(p)=300
[32m2023-10-20T13:47:31 | tasks.shared_utils: [0mLoading checkpoint from /home/wiss/zhang/nfs/anetqa_train_qa_full/ckpt_best.pth
[32m2023-10-20T13:48:01 | models.utils: [0mLoad temporal_embeddings, lengths: 4-->4
[32m2023-10-20T13:48:01 | models.utils: [0mLoad temporal_embeddings, lengths: 4-->4
[32m2023-10-20T13:48:01 | tasks.shared_utils: [0m_IncompatibleKeys(missing_keys=['temp', 'vision_encoder.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.pooler.layernorm.weight', 'vision_encoder.pooler.layernorm.bias', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'], unexpected_keys=['text_decoder.cls.predictions.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.cls.predictions.decoder.bias', 'text_decoder.embeddings.position_ids', 'text_decoder.embeddings.word_embeddings.weight', 'text_decoder.embeddings.position_embeddings.weight', 'text_decoder.embeddings.token_type_embeddings.weight', 'text_decoder.embeddings.LayerNorm.weight', 'text_decoder.embeddings.LayerNorm.bias', 'text_decoder.encoder.layer.0.attention.self.query.weight', 'text_decoder.encoder.layer.0.attention.self.query.bias', 'text_decoder.encoder.layer.0.attention.self.key.weight', 'text_decoder.encoder.layer.0.attention.self.key.bias', 'text_decoder.encoder.layer.0.attention.self.value.weight', 'text_decoder.encoder.layer.0.attention.self.value.bias', 'text_decoder.encoder.layer.0.attention.output.dense.weight', 'text_decoder.encoder.layer.0.attention.output.dense.bias', 'text_decoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.encoder.layer.0.intermediate.dense.weight', 'text_decoder.encoder.layer.0.intermediate.dense.bias', 'text_decoder.encoder.layer.0.output.dense.weight', 'text_decoder.encoder.layer.0.output.dense.bias', 'text_decoder.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.encoder.layer.1.attention.self.query.weight', 'text_decoder.encoder.layer.1.attention.self.query.bias', 'text_decoder.encoder.layer.1.attention.self.key.weight', 'text_decoder.encoder.layer.1.attention.self.key.bias', 'text_decoder.encoder.layer.1.attention.self.value.weight', 'text_decoder.encoder.layer.1.attention.self.value.bias', 'text_decoder.encoder.layer.1.attention.output.dense.weight', 'text_decoder.encoder.layer.1.attention.output.dense.bias', 'text_decoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.encoder.layer.1.intermediate.dense.weight', 'text_decoder.encoder.layer.1.intermediate.dense.bias', 'text_decoder.encoder.layer.1.output.dense.weight', 'text_decoder.encoder.layer.1.output.dense.bias', 'text_decoder.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.encoder.layer.2.attention.self.query.weight', 'text_decoder.encoder.layer.2.attention.self.query.bias', 'text_decoder.encoder.layer.2.attention.self.key.weight', 'text_decoder.encoder.layer.2.attention.self.key.bias', 'text_decoder.encoder.layer.2.attention.self.value.weight', 'text_decoder.encoder.layer.2.attention.self.value.bias', 'text_decoder.encoder.layer.2.attention.output.dense.weight', 'text_decoder.encoder.layer.2.attention.output.dense.bias', 'text_decoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.encoder.layer.2.intermediate.dense.weight', 'text_decoder.encoder.layer.2.intermediate.dense.bias', 'text_decoder.encoder.layer.2.output.dense.weight', 'text_decoder.encoder.layer.2.output.dense.bias', 'text_decoder.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.encoder.layer.2.output.LayerNorm.bias'])
[32m2023-10-20T13:48:01 | tasks.shared_utils: [0mLoaded checkpoint from /home/wiss/zhang/nfs/anetqa_train_qa_full/ckpt_best.pth
[32m2023-10-20T13:48:01 | __main__: [0mtraining
[32m2023-10-20T13:48:07 | dataset.dataloader: [0mMetaLoader has 1 dataloaders, 4 batches in total
dataloader index=0 name=video, batch-size=4 length(#batches)=4 
idx in forward tensor([3, 2, 2, 1], device='cuda:0')
text in forward torch.Size([4, 60, 768])
pooled_text_embeds in forward torch.Size([4, 768])
image in forward torch.Size([4, 788, 768])
pooled_image_embeds in forward torch.Size([4, 4, 768])
neg_text1_embeds in forward torch.Size([4, 60, 768])
text_feat in get_sim torch.Size([4, 256])
image_feat in get_sim torch.Size([4, 4, 256])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
extended_sim_matrix in get_contrastive_loss tensor([[ 0.0252,  0.0264, -0.0067,  0.0410,  0.0476,  0.0281,  0.0501, -0.0082,
         -0.0304,  0.0085,  0.0181],
        [-0.0139, -0.0093, -0.0115, -0.0337, -0.0117, -0.0160, -0.0266, -0.0252,
         -0.0003, -0.0091, -0.0138],
        [ 0.0290,  0.0298,  0.0390,  0.0095, -0.0115,  0.0041, -0.0302,  0.0230,
         -0.0129,  0.0039,  0.0215],
        [ 0.0143,  0.0258,  0.0424,  0.0454, -0.0035, -0.0317, -0.0251,  0.0024,
          0.0425,  0.0268,  0.0118]], device='cuda:0', dtype=torch.float16,
       grad_fn=<CatBackward0>)
max_value in get_contrastive_loss tensor([ 0.0501, -0.0003,  0.0390,  0.0454], device='cuda:0',
       dtype=torch.float16, grad_fn=<MaxBackward0>)
max_value in get_contrastive_loss torch.Size([4])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0248, -0.0237, -0.0568, -0.0091, -0.0024, -0.0220,  0.0000, -0.0583,
         -0.0805, -0.0416, -0.0320],
        [-0.0136, -0.0091, -0.0112, -0.0334, -0.0115, -0.0158, -0.0264, -0.0249,
          0.0000, -0.0088, -0.0136],
        [-0.0100, -0.0091,  0.0000, -0.0294, -0.0504, -0.0348, -0.0692, -0.0160,
         -0.0518, -0.0350, -0.0174],
        [-0.0311, -0.0196, -0.0031,  0.0000, -0.0490, -0.0771, -0.0705, -0.0430,
         -0.0030, -0.0187, -0.0336]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>)
softmax_vt in get_contrastive_loss tensor([[0.9755, 0.9766, 0.9448, 0.9910, 0.9976, 0.9783, 1.0000, 0.9434, 0.9226,
         0.9592, 0.9685],
        [0.9865, 0.9910, 0.9888, 0.9671, 0.9886, 0.9843, 0.9740, 0.9754, 1.0000,
         0.9912, 0.9865],
        [0.9901, 0.9909, 1.0000, 0.9710, 0.9508, 0.9658, 0.9332, 0.9841, 0.9495,
         0.9656, 0.9827],
        [0.9693, 0.9806, 0.9969, 1.0000, 0.9522, 0.9258, 0.9319, 0.9579, 0.9970,
         0.9815, 0.9669]], device='cuda:0', grad_fn=<ExpBackward0>)
part_1 in get_contrastive_loss tensor([3.8878, 3.9334, 3.9520, 3.9469], device='cuda:0',
       grad_fn=<SumBackward1>)
part_2 in get_contrastive_loss tensor([5.7721, 5.9114, 5.7809, 5.7611], device='cuda:0',
       grad_fn=<SumBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare tensor([[ 0.0252,  0.0264, -0.0067,  0.0410],
        [-0.0139, -0.0093, -0.0115, -0.0337],
        [ 0.0290,  0.0298,  0.0390,  0.0095],
        [ 0.0143,  0.0258,  0.0424,  0.0454]], device='cuda:0',
       dtype=torch.float16, grad_fn=<MeanBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare torch.Size([4, 4])
sim_i2t in get_contrastive_loss tensor([[-0.0248, -0.0237, -0.0568, -0.0091],
        [-0.0136, -0.0091, -0.0112, -0.0334],
        [-0.0100, -0.0091,  0.0000, -0.0294],
        [-0.0311, -0.0196, -0.0031,  0.0000]], device='cuda:0',
       dtype=torch.float16, grad_fn=<SubBackward0>)
loss_i2t_ori in get_contrastive_loss tensor(1.3773, device='cuda:0', grad_fn=<NegBackward0>)
loss_i2t in get_contrastive_loss tensor(1.3771, device='cuda:0', grad_fn=<MeanBackward0>)
[5m[31mWARNING[0m [32m2023-10-20T13:48:12 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

[5m[31mWARNING[0m [32m2023-10-20T13:48:12 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

[32m2023-10-20T13:48:12 | utils.basic_utils: [0mTrain Epoch: [0]  [0/4]  eta: 0:00:19  lr: 0.000010  temperature: 0.0100  video-loss_ita: 1.6619  video-loss_itm: 0.6553  time: 4.9490  data: 1.5155  max mem: 5661 res mem: 6102
idx in forward tensor([2, 2, 3, 1], device='cuda:0')
text in forward torch.Size([4, 60, 768])
pooled_text_embeds in forward torch.Size([4, 768])
image in forward torch.Size([4, 788, 768])
pooled_image_embeds in forward torch.Size([4, 4, 768])
neg_text1_embeds in forward torch.Size([4, 60, 768])
text_feat in get_sim torch.Size([4, 256])
image_feat in get_sim torch.Size([4, 4, 256])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0021, -0.0139,  0.0011, -0.0145, -0.0413, -0.0119, -0.0389, -0.0150,
          0.0120,  0.0178, -0.0104],
        [-0.0197, -0.0278, -0.0072, -0.0204, -0.0318, -0.0487, -0.0445,  0.0101,
          0.0259, -0.0053,  0.0120],
        [-0.0082,  0.0063,  0.0019,  0.0302,  0.0216, -0.0082,  0.0216, -0.0043,
         -0.0006, -0.0144, -0.0061],
        [ 0.0441,  0.0369,  0.0118,  0.0148, -0.0166, -0.0397, -0.0261,  0.0221,
         -0.0064,  0.0423,  0.0037]], device='cuda:0', dtype=torch.float16,
       grad_fn=<CatBackward0>)
max_value in get_contrastive_loss tensor([0.0178, 0.0259, 0.0302, 0.0441], device='cuda:0', dtype=torch.float16,
       grad_fn=<MaxBackward0>)
max_value in get_contrastive_loss torch.Size([4])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0200, -0.0317, -0.0167, -0.0323, -0.0591, -0.0298, -0.0567, -0.0328,
         -0.0059,  0.0000, -0.0283],
        [-0.0455, -0.0537, -0.0330, -0.0463, -0.0577, -0.0746, -0.0704, -0.0158,
          0.0000, -0.0312, -0.0138],
        [-0.0383, -0.0239, -0.0283,  0.0000, -0.0086, -0.0384, -0.0086, -0.0345,
         -0.0308, -0.0446, -0.0363],
        [ 0.0000, -0.0072, -0.0323, -0.0293, -0.0607, -0.0837, -0.0702, -0.0220,
         -0.0505, -0.0018, -0.0404]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>)
softmax_vt in get_contrastive_loss tensor([[0.9802, 0.9688, 0.9834, 0.9682, 0.9426, 0.9707, 0.9449, 0.9678, 0.9942,
         1.0000, 0.9721],
        [0.9555, 0.9477, 0.9675, 0.9548, 0.9440, 0.9281, 0.9320, 0.9843, 1.0000,
         0.9693, 0.9862],
        [0.9624, 0.9764, 0.9721, 1.0000, 0.9915, 0.9623, 0.9915, 0.9661, 0.9697,
         0.9564, 0.9643],
        [1.0000, 0.9928, 0.9682, 0.9711, 0.9411, 0.9197, 0.9322, 0.9783, 0.9507,
         0.9982, 0.9604]], device='cuda:0', grad_fn=<ExpBackward0>)
part_1 in get_contrastive_loss tensor([3.9007, 3.8255, 3.9109, 3.9322], device='cuda:0',
       grad_fn=<SumBackward1>)
part_2 in get_contrastive_loss tensor([5.8496, 5.8001, 5.8104, 5.7395], device='cuda:0',
       grad_fn=<SumBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare tensor([[-0.0021, -0.0139,  0.0011, -0.0145],
        [-0.0197, -0.0278, -0.0072, -0.0204],
        [-0.0082,  0.0063,  0.0019,  0.0302],
        [ 0.0441,  0.0369,  0.0118,  0.0148]], device='cuda:0',
       dtype=torch.float16, grad_fn=<MeanBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare torch.Size([4, 4])
sim_i2t in get_contrastive_loss tensor([[-0.0200, -0.0317, -0.0167, -0.0323],
        [-0.0455, -0.0537, -0.0330, -0.0463],
        [-0.0383, -0.0239, -0.0283,  0.0000],
        [ 0.0000, -0.0072, -0.0323, -0.0293]], device='cuda:0',
       dtype=torch.float16, grad_fn=<SubBackward0>)
loss_i2t_ori in get_contrastive_loss tensor(1.3916, device='cuda:0', grad_fn=<NegBackward0>)
loss_i2t in get_contrastive_loss tensor(1.3918, device='cuda:0', grad_fn=<MeanBackward0>)
idx in forward tensor([0, 3, 3, 0], device='cuda:0')
text in forward torch.Size([4, 60, 768])
pooled_text_embeds in forward torch.Size([4, 768])
image in forward torch.Size([4, 788, 768])
pooled_image_embeds in forward torch.Size([4, 4, 768])
neg_text1_embeds in forward torch.Size([4, 60, 768])
text_feat in get_sim torch.Size([4, 256])
image_feat in get_sim torch.Size([4, 4, 256])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0247, -0.0054, -0.0153, -0.0086, -0.0858, -0.0462, -0.0378, -0.0332,
         -0.0664, -0.0229, -0.0291],
        [ 0.0002, -0.0081,  0.0048,  0.0101,  0.0313,  0.0056, -0.0105, -0.0412,
         -0.0051,  0.0005, -0.0055],
        [-0.0043, -0.0159, -0.0088,  0.0008,  0.0221,  0.0223,  0.0329, -0.0060,
         -0.0386, -0.0065, -0.0216],
        [ 0.0032,  0.0294,  0.0213,  0.0147, -0.0001, -0.0112, -0.0208, -0.0079,
         -0.0132, -0.0199, -0.0057]], device='cuda:0', dtype=torch.float16,
       grad_fn=<CatBackward0>)
max_value in get_contrastive_loss tensor([-0.0054,  0.0313,  0.0329,  0.0294], device='cuda:0',
       dtype=torch.float16, grad_fn=<MaxBackward0>)
max_value in get_contrastive_loss torch.Size([4])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0193,  0.0000, -0.0099, -0.0032, -0.0804, -0.0409, -0.0324, -0.0279,
         -0.0610, -0.0176, -0.0238],
        [-0.0311, -0.0395, -0.0265, -0.0212,  0.0000, -0.0257, -0.0418, -0.0726,
         -0.0364, -0.0308, -0.0368],
        [-0.0372, -0.0488, -0.0417, -0.0321, -0.0108, -0.0106,  0.0000, -0.0389,
         -0.0715, -0.0394, -0.0545],
        [-0.0262,  0.0000, -0.0080, -0.0147, -0.0295, -0.0406, -0.0501, -0.0372,
         -0.0425, -0.0492, -0.0350]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>)
softmax_vt in get_contrastive_loss tensor([[0.9809, 1.0000, 0.9901, 0.9968, 0.9228, 0.9600, 0.9681, 0.9725, 0.9408,
         0.9826, 0.9765],
        [0.9694, 0.9613, 0.9738, 0.9790, 1.0000, 0.9746, 0.9590, 0.9300, 0.9642,
         0.9696, 0.9639],
        [0.9635, 0.9524, 0.9592, 0.9684, 0.9892, 0.9895, 1.0000, 0.9618, 0.9310,
         0.9614, 0.9470],
        [0.9742, 1.0000, 0.9920, 0.9854, 0.9710, 0.9603, 0.9511, 0.9635, 0.9584,
         0.9520, 0.9656]], device='cuda:0', grad_fn=<ExpBackward0>)
part_1 in get_contrastive_loss tensor([3.9678, 3.8835, 3.8434, 3.9516], device='cuda:0',
       grad_fn=<SumBackward1>)
part_2 in get_contrastive_loss tensor([5.8005, 5.7614, 5.7906, 5.7508], device='cuda:0',
       grad_fn=<SumBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare tensor([[-0.0247, -0.0054, -0.0153, -0.0086],
        [ 0.0002, -0.0081,  0.0048,  0.0101],
        [-0.0043, -0.0159, -0.0088,  0.0008],
        [ 0.0032,  0.0294,  0.0213,  0.0147]], device='cuda:0',
       dtype=torch.float16, grad_fn=<MeanBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare torch.Size([4, 4])
sim_i2t in get_contrastive_loss tensor([[-0.0193,  0.0000, -0.0099, -0.0032],
        [-0.0311, -0.0395, -0.0265, -0.0212],
        [-0.0372, -0.0488, -0.0417, -0.0321],
        [-0.0262,  0.0000, -0.0080, -0.0147]], device='cuda:0',
       dtype=torch.float16, grad_fn=<SubBackward0>)
loss_i2t_ori in get_contrastive_loss tensor(1.3925, device='cuda:0', grad_fn=<NegBackward0>)
loss_i2t in get_contrastive_loss tensor(1.3926, device='cuda:0', grad_fn=<MeanBackward0>)
idx in forward tensor([1, 0, 1, 0], device='cuda:0')
text in forward torch.Size([4, 60, 768])
pooled_text_embeds in forward torch.Size([4, 768])
image in forward torch.Size([4, 788, 768])
pooled_image_embeds in forward torch.Size([4, 4, 768])
neg_text1_embeds in forward torch.Size([4, 60, 768])
text_feat in get_sim torch.Size([4, 256])
image_feat in get_sim torch.Size([4, 4, 256])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
pos_iamge_feat in get_contrastive_loss torch.Size([1, 4, 256])
sim_i2t_pos in get_contrastive_loss torch.Size([1, 4])
sim_i2t_neg in get_contrastive_loss torch.Size([1, 7])
sim_i2t_extended in get_contrastive_loss torch.Size([1, 11])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0006,  0.0304,  0.0180,  0.0207, -0.0389, -0.0480, -0.0353,  0.0044,
          0.0104,  0.0082, -0.0108],
        [ 0.0082,  0.0286,  0.0020,  0.0128, -0.0290,  0.0141, -0.0345, -0.0001,
         -0.0375, -0.0206, -0.0401],
        [ 0.0075,  0.0288,  0.0271,  0.0264, -0.0106, -0.0170,  0.0043,  0.0178,
          0.0084, -0.0055, -0.0028],
        [-0.0064,  0.0139, -0.0305,  0.0064, -0.0115, -0.0427, -0.0167,  0.0174,
          0.0171, -0.0356, -0.0143]], device='cuda:0', dtype=torch.float16,
       grad_fn=<CatBackward0>)
max_value in get_contrastive_loss tensor([0.0304, 0.0286, 0.0288, 0.0174], device='cuda:0', dtype=torch.float16,
       grad_fn=<MaxBackward0>)
max_value in get_contrastive_loss torch.Size([4])
extended_sim_matrix in get_contrastive_loss tensor([[-0.0311,  0.0000, -0.0124, -0.0097, -0.0693, -0.0784, -0.0657, -0.0260,
         -0.0200, -0.0222, -0.0412],
        [-0.0204,  0.0000, -0.0266, -0.0158, -0.0576, -0.0146, -0.0631, -0.0287,
         -0.0661, -0.0493, -0.0687],
        [-0.0213,  0.0000, -0.0017, -0.0024, -0.0394, -0.0457, -0.0245, -0.0110,
         -0.0204, -0.0343, -0.0316],
        [-0.0238, -0.0035, -0.0479, -0.0110, -0.0288, -0.0601, -0.0340,  0.0000,
         -0.0002, -0.0529, -0.0316]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>)
softmax_vt in get_contrastive_loss tensor([[0.9694, 1.0000, 0.9877, 0.9903, 0.9331, 0.9246, 0.9364, 0.9743, 0.9802,
         0.9780, 0.9596],
        [0.9799, 1.0000, 0.9738, 0.9843, 0.9440, 0.9855, 0.9388, 0.9717, 0.9360,
         0.9519, 0.9336],
        [0.9790, 1.0000, 0.9983, 0.9976, 0.9614, 0.9553, 0.9758, 0.9891, 0.9799,
         0.9663, 0.9689],
        [0.9765, 0.9965, 0.9533, 0.9891, 0.9716, 0.9417, 0.9665, 1.0000, 0.9998,
         0.9485, 0.9688]], device='cuda:0', grad_fn=<ExpBackward0>)
part_1 in get_contrastive_loss tensor([3.9474, 3.9379, 3.9749, 3.9154], device='cuda:0',
       grad_fn=<SumBackward1>)
part_2 in get_contrastive_loss tensor([5.7532, 5.7176, 5.8352, 5.8253], device='cuda:0',
       grad_fn=<SumBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare tensor([[-0.0006,  0.0304,  0.0180,  0.0207],
        [ 0.0082,  0.0286,  0.0020,  0.0128],
        [ 0.0075,  0.0288,  0.0271,  0.0264],
        [-0.0064,  0.0139, -0.0305,  0.0064]], device='cuda:0',
       dtype=torch.float16, grad_fn=<MeanBackward1>)
sim_i2t_inbatch in get_contrastive_loss as compare torch.Size([4, 4])
sim_i2t in get_contrastive_loss tensor([[-0.0311,  0.0000, -0.0124, -0.0097],
        [-0.0204,  0.0000, -0.0266, -0.0158],
        [-0.0213,  0.0000, -0.0017, -0.0024],
        [-0.0238, -0.0035, -0.0479, -0.0110]], device='cuda:0',
       dtype=torch.float16, grad_fn=<SubBackward0>)
loss_i2t_ori in get_contrastive_loss tensor(1.3831, device='cuda:0', grad_fn=<NegBackward0>)
loss_i2t in get_contrastive_loss tensor(1.3831, device='cuda:0', grad_fn=<MeanBackward0>)
[32m2023-10-20T13:48:13 | utils.basic_utils: [0mTrain Epoch: [0]  [3/4]  eta: 0:00:01  lr: 0.000010  temperature: 0.0100  video-loss_ita: 1.5166  video-loss_itm: 0.6470  time: 1.7043  data: 0.3789  max mem: 6514 res mem: 6938
[32m2023-10-20T13:48:13 | utils.basic_utils: [0mTrain Epoch: [0] Total time: 0:00:06 (1.7050 s / it)
[32m2023-10-20T13:48:13 | __main__: [0mAveraged train stats: lr: 0.0000  temperature: 0.0100  video-loss_ita: 1.9406  video-loss_itm: 0.6555
[32m2023-10-20T13:48:13 | tasks.retrieval_utils: [0mStart evaluation for media_type=video
[32m2023-10-20T13:48:13 | tasks.retrieval_utils: [0mComputing dual encoder features...
[32m2023-10-20T13:48:40 | utils.basic_utils: [0mextracting image feats  [0/6]  eta: 0:02:39    time: 26.6450  data: 25.2602  max mem: 6514 res mem: 7004
[32m2023-10-20T13:48:45 | utils.basic_utils: [0mextracting image feats  [5/6]  eta: 0:00:05    time: 5.2253  data: 4.2101  max mem: 6514 res mem: 7004
[32m2023-10-20T13:48:45 | utils.basic_utils: [0mextracting image feats Total time: 0:00:31 (5.2257 s / it)
[32m2023-10-20T13:48:45 | tasks.retrieval_utils: [0mFinished feature extraction
[32m2023-10-20T13:48:45 | tasks.retrieval_utils: [0mComputing ITC scores [dot-product]
text_feat in get_sim torch.Size([184, 256])
image_feat in get_sim torch.Size([184, 4, 256])
[32m2023-10-20T13:48:45 | tasks.retrieval_utils: [0mComputing ITC scores [dot-product], done!
[32m2023-10-20T13:48:45 | tasks.retrieval_utils: [0mRerank dual-encoder results with cross-encoder...
[32m2023-10-20T13:48:45 | tasks.retrieval_utils: [0mi2t_scores.shape torch.Size([184, 184])
[32m2023-10-20T13:48:45 | tasks.retrieval_utils: [0mn_clip_per_video=1, with eval_frame_ensemble=concat
[32m2023-10-20T13:48:45 | utils.basic_utils: [0mEvaluation:  [  0/184]  eta: 0:00:03    time: 0.0196  data: 0.0003  max mem: 6514 res mem: 7004
[32m2023-10-20T13:49:07 | utils.basic_utils: [0mEvaluation:  [100/184]  eta: 0:00:18    time: 0.2195  data: 0.0000  max mem: 6514 res mem: 7004
