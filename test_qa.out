rdzv_endpoint: worker-4:40000
[32m2023-10-18T23:35:21 | loopitr: [0mLogging to: /home/wiss/zhang/Jinhe/singularity/test_model/model_test_Qa/train.log
[32m2023-10-18T23:35:21 | __main__: [0mconfig: 
{'dataset_name': 'anet', 'data_root': '/home/wiss/zhang/nfs/Anet_sing', 'anno_root_downstream': '/home/wiss/zhang/Jinhe/singularity/anno_downstream_ori', 'train_file': [['/nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json', '/home/wiss/zhang/nfs/Anet_sing', 'video']], 'test_types': ['val'], 'test_file': {'val': ['/nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'test': ['/nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json', '/home/wiss/zhang/nfs/Anet_sing', 'video']}, 'stop_key': 'val', 'answer_list': '/nfs/data2/zhang/AnetQA/qa_train/answer_list_light.json', 'text_encoder': 'bert-base-uncased', 'text_decoder': 'bert-base-uncased', 'bert_config': 'configs/config_bert.json', 'vit_type': 'beit', 'vit_zoo': {'beit': 'microsoft/beit-base-patch16-224-pt22k-ft22k'}, 'vit_name_or_pretrained_path': '${vit_zoo[${vit_type}]}', 'temporal_vision_encoder': {'enable': True, 'num_layers': 2, 'update_pooler_embed': False}, 'add_temporal_embed': True, 'image_res': 224, 'embed_dim': 256, 'video_input': {'num_frames': 1, 'reader': 'decord', 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle'}, 'batch_size': {'image': 128, 'video': 64}, 'batch_size_test': {'image': 64, 'video': 64}, 'k_test': 128, 'temp': 0.07, 'eos': '[SEP]', 'max_q_len': 25, 'max_a_len': 5, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 10, 'min_lr_multi': 0.1, 'warmup_epochs': 0.5}, 'output_dir': '/home/wiss/zhang/Jinhe/singularity/test_model/model_test_Qa', 'pretrained_path': '/home/wiss/zhang/nfs/anetqa_train_qa_full/ckpt_best.pth', 'resume': False, 'evaluate': True, 'eval_frame_ensemble': 'concat', 'device': 'cuda', 'seed': 42, 'log_freq': 100, 'dist_url': 'env://', 'distributed': True, 'fp16': True, 'debug': False, 'num_workers': 24, 'wandb': {'enable': False, 'entity': 'gengyuanzhang', 'project': 'vqa'}, 'rank': 0, 'world_size': 1, 'gpu': 0, 'dist_backend': 'nccl', 'result_dir': '/home/wiss/zhang/Jinhe/singularity/test_model/model_test_Qa'}
[32m2023-10-18T23:35:21 | __main__: [0mtrain_file: [['/nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json', '/home/wiss/zhang/nfs/Anet_sing', 'video']]
[32m2023-10-18T23:35:21 | __main__: [0mCreating vqa QA datasets
[5m[31mWARNING[0m [32m2023-10-18T23:35:22 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  builtin_warn(*args, **kwargs)

[5m[31mWARNING[0m [32m2023-10-18T23:35:22 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  builtin_warn(*args, **kwargs)

[32m2023-10-18T23:35:22 | tasks.shared_utils: [0mCreating model
[32m2023-10-18T23:35:26 | models.model_retrieval_base: [0mLoading vit pre-trained weights from huggingface microsoft/beit-base-patch16-224-pt22k-ft22k.
[5m[31mWARNING[0m [32m2023-10-18T23:35:29 | py.warnings: [0m/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[5m[31mWARNING[0m [32m2023-10-18T23:35:29 | py.warnings: [0m/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[32m2023-10-18T23:35:30 | models.model_retrieval_base: [0mInit new model with new image size 224, and load weights.
[32m2023-10-18T23:35:31 | models.model_retrieval_base: [0m_IncompatibleKeys(missing_keys=['encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.11.attention.attention.relative_position_bias.relative_position_index'], unexpected_keys=[])
[32m2023-10-18T23:35:31 | models.model_retrieval_base: [0mBuild text_encoder bert-base-uncased
[32m2023-10-18T23:35:34 | models.model_retrieval_base: [0mBuild text_encoder bert-base-uncased, done!
[32m2023-10-18T23:35:34 | models.model_retrieval_base: [0mBuild temporal_vision_encoder (#layer=2), randomly initialised.
[32m2023-10-18T23:35:34 | models.model_retrieval_base: [0mBuild temporal_vision_encoder, done!
[32m2023-10-18T23:35:34 | models.model_vqa: [0mBuild text_decoder bert-base-uncased
[32m2023-10-18T23:35:35 | models.model_vqa: [0mBuild text_decoder bert-base-uncased, done!
[32m2023-10-18T23:35:35 | utils.optimizer: [0moptimizer -- lr=1e-05 wd=0.02 len(p)=220
[32m2023-10-18T23:35:35 | utils.optimizer: [0moptimizer -- lr=1e-05 wd=0 len(p)=349
[32m2023-10-18T23:35:35 | tasks.shared_utils: [0mLoading checkpoint from /home/wiss/zhang/nfs/anetqa_train_qa_full/ckpt_best.pth
state_dict.keys():  odict_keys(['temporal_embeddings', 'vision_encoder.embeddings.cls_token', 'vision_encoder.embeddings.patch_embeddings.projection.weight', 'vision_encoder.embeddings.patch_embeddings.projection.bias', 'vision_encoder.encoder.layer.0.lambda_1', 'vision_encoder.encoder.layer.0.lambda_2', 'vision_encoder.encoder.layer.0.attention.attention.query.weight', 'vision_encoder.encoder.layer.0.attention.attention.query.bias', 'vision_encoder.encoder.layer.0.attention.attention.key.weight', 'vision_encoder.encoder.layer.0.attention.attention.value.weight', 'vision_encoder.encoder.layer.0.attention.attention.value.bias', 'vision_encoder.encoder.layer.0.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.0.attention.output.dense.weight', 'vision_encoder.encoder.layer.0.attention.output.dense.bias', 'vision_encoder.encoder.layer.0.intermediate.dense.weight', 'vision_encoder.encoder.layer.0.intermediate.dense.bias', 'vision_encoder.encoder.layer.0.output.dense.weight', 'vision_encoder.encoder.layer.0.output.dense.bias', 'vision_encoder.encoder.layer.0.layernorm_before.weight', 'vision_encoder.encoder.layer.0.layernorm_before.bias', 'vision_encoder.encoder.layer.0.layernorm_after.weight', 'vision_encoder.encoder.layer.0.layernorm_after.bias', 'vision_encoder.encoder.layer.1.lambda_1', 'vision_encoder.encoder.layer.1.lambda_2', 'vision_encoder.encoder.layer.1.attention.attention.query.weight', 'vision_encoder.encoder.layer.1.attention.attention.query.bias', 'vision_encoder.encoder.layer.1.attention.attention.key.weight', 'vision_encoder.encoder.layer.1.attention.attention.value.weight', 'vision_encoder.encoder.layer.1.attention.attention.value.bias', 'vision_encoder.encoder.layer.1.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.1.attention.output.dense.weight', 'vision_encoder.encoder.layer.1.attention.output.dense.bias', 'vision_encoder.encoder.layer.1.intermediate.dense.weight', 'vision_encoder.encoder.layer.1.intermediate.dense.bias', 'vision_encoder.encoder.layer.1.output.dense.weight', 'vision_encoder.encoder.layer.1.output.dense.bias', 'vision_encoder.encoder.layer.1.layernorm_before.weight', 'vision_encoder.encoder.layer.1.layernorm_before.bias', 'vision_encoder.encoder.layer.1.layernorm_after.weight', 'vision_encoder.encoder.layer.1.layernorm_after.bias', 'vision_encoder.encoder.layer.2.lambda_1', 'vision_encoder.encoder.layer.2.lambda_2', 'vision_encoder.encoder.layer.2.attention.attention.query.weight', 'vision_encoder.encoder.layer.2.attention.attention.query.bias', 'vision_encoder.encoder.layer.2.attention.attention.key.weight', 'vision_encoder.encoder.layer.2.attention.attention.value.weight', 'vision_encoder.encoder.layer.2.attention.attention.value.bias', 'vision_encoder.encoder.layer.2.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.2.attention.output.dense.weight', 'vision_encoder.encoder.layer.2.attention.output.dense.bias', 'vision_encoder.encoder.layer.2.intermediate.dense.weight', 'vision_encoder.encoder.layer.2.intermediate.dense.bias', 'vision_encoder.encoder.layer.2.output.dense.weight', 'vision_encoder.encoder.layer.2.output.dense.bias', 'vision_encoder.encoder.layer.2.layernorm_before.weight', 'vision_encoder.encoder.layer.2.layernorm_before.bias', 'vision_encoder.encoder.layer.2.layernorm_after.weight', 'vision_encoder.encoder.layer.2.layernorm_after.bias', 'vision_encoder.encoder.layer.3.lambda_1', 'vision_encoder.encoder.layer.3.lambda_2', 'vision_encoder.encoder.layer.3.attention.attention.query.weight', 'vision_encoder.encoder.layer.3.attention.attention.query.bias', 'vision_encoder.encoder.layer.3.attention.attention.key.weight', 'vision_encoder.encoder.layer.3.attention.attention.value.weight', 'vision_encoder.encoder.layer.3.attention.attention.value.bias', 'vision_encoder.encoder.layer.3.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.3.attention.output.dense.weight', 'vision_encoder.encoder.layer.3.attention.output.dense.bias', 'vision_encoder.encoder.layer.3.intermediate.dense.weight', 'vision_encoder.encoder.layer.3.intermediate.dense.bias', 'vision_encoder.encoder.layer.3.output.dense.weight', 'vision_encoder.encoder.layer.3.output.dense.bias', 'vision_encoder.encoder.layer.3.layernorm_before.weight', 'vision_encoder.encoder.layer.3.layernorm_before.bias', 'vision_encoder.encoder.layer.3.layernorm_after.weight', 'vision_encoder.encoder.layer.3.layernorm_after.bias', 'vision_encoder.encoder.layer.4.lambda_1', 'vision_encoder.encoder.layer.4.lambda_2', 'vision_encoder.encoder.layer.4.attention.attention.query.weight', 'vision_encoder.encoder.layer.4.attention.attention.query.bias', 'vision_encoder.encoder.layer.4.attention.attention.key.weight', 'vision_encoder.encoder.layer.4.attention.attention.value.weight', 'vision_encoder.encoder.layer.4.attention.attention.value.bias', 'vision_encoder.encoder.layer.4.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.4.attention.output.dense.weight', 'vision_encoder.encoder.layer.4.attention.output.dense.bias', 'vision_encoder.encoder.layer.4.intermediate.dense.weight', 'vision_encoder.encoder.layer.4.intermediate.dense.bias', 'vision_encoder.encoder.layer.4.output.dense.weight', 'vision_encoder.encoder.layer.4.output.dense.bias', 'vision_encoder.encoder.layer.4.layernorm_before.weight', 'vision_encoder.encoder.layer.4.layernorm_before.bias', 'vision_encoder.encoder.layer.4.layernorm_after.weight', 'vision_encoder.encoder.layer.4.layernorm_after.bias', 'vision_encoder.encoder.layer.5.lambda_1', 'vision_encoder.encoder.layer.5.lambda_2', 'vision_encoder.encoder.layer.5.attention.attention.query.weight', 'vision_encoder.encoder.layer.5.attention.attention.query.bias', 'vision_encoder.encoder.layer.5.attention.attention.key.weight', 'vision_encoder.encoder.layer.5.attention.attention.value.weight', 'vision_encoder.encoder.layer.5.attention.attention.value.bias', 'vision_encoder.encoder.layer.5.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.5.attention.output.dense.weight', 'vision_encoder.encoder.layer.5.attention.output.dense.bias', 'vision_encoder.encoder.layer.5.intermediate.dense.weight', 'vision_encoder.encoder.layer.5.intermediate.dense.bias', 'vision_encoder.encoder.layer.5.output.dense.weight', 'vision_encoder.encoder.layer.5.output.dense.bias', 'vision_encoder.encoder.layer.5.layernorm_before.weight', 'vision_encoder.encoder.layer.5.layernorm_before.bias', 'vision_encoder.encoder.layer.5.layernorm_after.weight', 'vision_encoder.encoder.layer.5.layernorm_after.bias', 'vision_encoder.encoder.layer.6.lambda_1', 'vision_encoder.encoder.layer.6.lambda_2', 'vision_encoder.encoder.layer.6.attention.attention.query.weight', 'vision_encoder.encoder.layer.6.attention.attention.query.bias', 'vision_encoder.encoder.layer.6.attention.attention.key.weight', 'vision_encoder.encoder.layer.6.attention.attention.value.weight', 'vision_encoder.encoder.layer.6.attention.attention.value.bias', 'vision_encoder.encoder.layer.6.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.6.attention.output.dense.weight', 'vision_encoder.encoder.layer.6.attention.output.dense.bias', 'vision_encoder.encoder.layer.6.intermediate.dense.weight', 'vision_encoder.encoder.layer.6.intermediate.dense.bias', 'vision_encoder.encoder.layer.6.output.dense.weight', 'vision_encoder.encoder.layer.6.output.dense.bias', 'vision_encoder.encoder.layer.6.layernorm_before.weight', 'vision_encoder.encoder.layer.6.layernorm_before.bias', 'vision_encoder.encoder.layer.6.layernorm_after.weight', 'vision_encoder.encoder.layer.6.layernorm_after.bias', 'vision_encoder.encoder.layer.7.lambda_1', 'vision_encoder.encoder.layer.7.lambda_2', 'vision_encoder.encoder.layer.7.attention.attention.query.weight', 'vision_encoder.encoder.layer.7.attention.attention.query.bias', 'vision_encoder.encoder.layer.7.attention.attention.key.weight', 'vision_encoder.encoder.layer.7.attention.attention.value.weight', 'vision_encoder.encoder.layer.7.attention.attention.value.bias', 'vision_encoder.encoder.layer.7.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.7.attention.output.dense.weight', 'vision_encoder.encoder.layer.7.attention.output.dense.bias', 'vision_encoder.encoder.layer.7.intermediate.dense.weight', 'vision_encoder.encoder.layer.7.intermediate.dense.bias', 'vision_encoder.encoder.layer.7.output.dense.weight', 'vision_encoder.encoder.layer.7.output.dense.bias', 'vision_encoder.encoder.layer.7.layernorm_before.weight', 'vision_encoder.encoder.layer.7.layernorm_before.bias', 'vision_encoder.encoder.layer.7.layernorm_after.weight', 'vision_encoder.encoder.layer.7.layernorm_after.bias', 'vision_encoder.encoder.layer.8.lambda_1', 'vision_encoder.encoder.layer.8.lambda_2', 'vision_encoder.encoder.layer.8.attention.attention.query.weight', 'vision_encoder.encoder.layer.8.attention.attention.query.bias', 'vision_encoder.encoder.layer.8.attention.attention.key.weight', 'vision_encoder.encoder.layer.8.attention.attention.value.weight', 'vision_encoder.encoder.layer.8.attention.attention.value.bias', 'vision_encoder.encoder.layer.8.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.8.attention.output.dense.weight', 'vision_encoder.encoder.layer.8.attention.output.dense.bias', 'vision_encoder.encoder.layer.8.intermediate.dense.weight', 'vision_encoder.encoder.layer.8.intermediate.dense.bias', 'vision_encoder.encoder.layer.8.output.dense.weight', 'vision_encoder.encoder.layer.8.output.dense.bias', 'vision_encoder.encoder.layer.8.layernorm_before.weight', 'vision_encoder.encoder.layer.8.layernorm_before.bias', 'vision_encoder.encoder.layer.8.layernorm_after.weight', 'vision_encoder.encoder.layer.8.layernorm_after.bias', 'vision_encoder.encoder.layer.9.lambda_1', 'vision_encoder.encoder.layer.9.lambda_2', 'vision_encoder.encoder.layer.9.attention.attention.query.weight', 'vision_encoder.encoder.layer.9.attention.attention.query.bias', 'vision_encoder.encoder.layer.9.attention.attention.key.weight', 'vision_encoder.encoder.layer.9.attention.attention.value.weight', 'vision_encoder.encoder.layer.9.attention.attention.value.bias', 'vision_encoder.encoder.layer.9.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.9.attention.output.dense.weight', 'vision_encoder.encoder.layer.9.attention.output.dense.bias', 'vision_encoder.encoder.layer.9.intermediate.dense.weight', 'vision_encoder.encoder.layer.9.intermediate.dense.bias', 'vision_encoder.encoder.layer.9.output.dense.weight', 'vision_encoder.encoder.layer.9.output.dense.bias', 'vision_encoder.encoder.layer.9.layernorm_before.weight', 'vision_encoder.encoder.layer.9.layernorm_before.bias', 'vision_encoder.encoder.layer.9.layernorm_after.weight', 'vision_encoder.encoder.layer.9.layernorm_after.bias', 'vision_encoder.encoder.layer.10.lambda_1', 'vision_encoder.encoder.layer.10.lambda_2', 'vision_encoder.encoder.layer.10.attention.attention.query.weight', 'vision_encoder.encoder.layer.10.attention.attention.query.bias', 'vision_encoder.encoder.layer.10.attention.attention.key.weight', 'vision_encoder.encoder.layer.10.attention.attention.value.weight', 'vision_encoder.encoder.layer.10.attention.attention.value.bias', 'vision_encoder.encoder.layer.10.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.10.attention.output.dense.weight', 'vision_encoder.encoder.layer.10.attention.output.dense.bias', 'vision_encoder.encoder.layer.10.intermediate.dense.weight', 'vision_encoder.encoder.layer.10.intermediate.dense.bias', 'vision_encoder.encoder.layer.10.output.dense.weight', 'vision_encoder.encoder.layer.10.output.dense.bias', 'vision_encoder.encoder.layer.10.layernorm_before.weight', 'vision_encoder.encoder.layer.10.layernorm_before.bias', 'vision_encoder.encoder.layer.10.layernorm_after.weight', 'vision_encoder.encoder.layer.10.layernorm_after.bias', 'vision_encoder.encoder.layer.11.lambda_1', 'vision_encoder.encoder.layer.11.lambda_2', 'vision_encoder.encoder.layer.11.attention.attention.query.weight', 'vision_encoder.encoder.layer.11.attention.attention.query.bias', 'vision_encoder.encoder.layer.11.attention.attention.key.weight', 'vision_encoder.encoder.layer.11.attention.attention.value.weight', 'vision_encoder.encoder.layer.11.attention.attention.value.bias', 'vision_encoder.encoder.layer.11.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.11.attention.output.dense.weight', 'vision_encoder.encoder.layer.11.attention.output.dense.bias', 'vision_encoder.encoder.layer.11.intermediate.dense.weight', 'vision_encoder.encoder.layer.11.intermediate.dense.bias', 'vision_encoder.encoder.layer.11.output.dense.weight', 'vision_encoder.encoder.layer.11.output.dense.bias', 'vision_encoder.encoder.layer.11.layernorm_before.weight', 'vision_encoder.encoder.layer.11.layernorm_before.bias', 'vision_encoder.encoder.layer.11.layernorm_after.weight', 'vision_encoder.encoder.layer.11.layernorm_after.bias', 'vision_layernorm.weight', 'vision_layernorm.bias', 'text_encoder.embeddings.position_ids', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.token_type_embeddings.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'temporal_vision_encoder.layer.0.attention.self.query.weight', 'temporal_vision_encoder.layer.0.attention.self.query.bias', 'temporal_vision_encoder.layer.0.attention.self.key.weight', 'temporal_vision_encoder.layer.0.attention.self.key.bias', 'temporal_vision_encoder.layer.0.attention.self.value.weight', 'temporal_vision_encoder.layer.0.attention.self.value.bias', 'temporal_vision_encoder.layer.0.attention.output.dense.weight', 'temporal_vision_encoder.layer.0.attention.output.dense.bias', 'temporal_vision_encoder.layer.0.attention.output.LayerNorm.weight', 'temporal_vision_encoder.layer.0.attention.output.LayerNorm.bias', 'temporal_vision_encoder.layer.0.intermediate.dense.weight', 'temporal_vision_encoder.layer.0.intermediate.dense.bias', 'temporal_vision_encoder.layer.0.output.dense.weight', 'temporal_vision_encoder.layer.0.output.dense.bias', 'temporal_vision_encoder.layer.0.output.LayerNorm.weight', 'temporal_vision_encoder.layer.0.output.LayerNorm.bias', 'temporal_vision_encoder.layer.1.attention.self.query.weight', 'temporal_vision_encoder.layer.1.attention.self.query.bias', 'temporal_vision_encoder.layer.1.attention.self.key.weight', 'temporal_vision_encoder.layer.1.attention.self.key.bias', 'temporal_vision_encoder.layer.1.attention.self.value.weight', 'temporal_vision_encoder.layer.1.attention.self.value.bias', 'temporal_vision_encoder.layer.1.attention.output.dense.weight', 'temporal_vision_encoder.layer.1.attention.output.dense.bias', 'temporal_vision_encoder.layer.1.attention.output.LayerNorm.weight', 'temporal_vision_encoder.layer.1.attention.output.LayerNorm.bias', 'temporal_vision_encoder.layer.1.intermediate.dense.weight', 'temporal_vision_encoder.layer.1.intermediate.dense.bias', 'temporal_vision_encoder.layer.1.output.dense.weight', 'temporal_vision_encoder.layer.1.output.dense.bias', 'temporal_vision_encoder.layer.1.output.LayerNorm.weight', 'temporal_vision_encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.embeddings.position_ids', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_decoder.bert.embeddings.token_type_embeddings.weight', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.cls.predictions.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.cls.predictions.decoder.bias'])
[32m2023-10-18T23:35:50 | models.utils: [0mLoad temporal_embeddings, lengths: 4-->1
[32m2023-10-18T23:35:50 | tasks.shared_utils: [0m<All keys matched successfully>
[32m2023-10-18T23:35:50 | tasks.shared_utils: [0mLoaded checkpoint from /home/wiss/zhang/nfs/anetqa_train_qa_full/ckpt_best.pth
[32m2023-10-18T23:35:50 | __main__: [0mStart evaluation
[32m2023-10-18T23:35:50 | __main__: [0mEvaluating val split...
[32m2023-10-18T23:35:51 | __main__: [0mStart generating results.
