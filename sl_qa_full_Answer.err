WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
wandb: Currently logged in as: gengyuanzhang (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run anetqa_train_qa_full
wandb:  View project at https://wandb.ai/gengyuanzhang/sb_qa_anet
wandb:  View run at https://wandb.ai/gengyuanzhang/sb_qa_anet/runs/yodr17fq
wandb: Run data is saved locally in /home/wiss/zhang/Jinhe/singularity/wandb/run-20231018_130833-yodr17fq
wandb: Run `wandb offline` to turn off syncing.
Loading /nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json:   0% 0/32050 [00:00<?, ?it/s]Loading /nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json:  71% 22631/32050 [00:00<00:00, 226292.41it/s]Loading /nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json: 100% 32050/32050 [00:00<00:00, 212661.61it/s]
Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json:   0% 0/4695 [00:00<?, ?it/s]Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json: 100% 4695/4695 [00:00<00:00, 128524.44it/s]
Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json:   0% 0/4695 [00:00<?, ?it/s]Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json: 100% 4695/4695 [00:00<00:00, 232502.42it/s]
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
wandb: Waiting for W&B process to finish, PID 1763728... (success).
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.89MB uploaded (0.00MB deduped)wandb: / 0.11MB of 0.89MB uploaded (0.00MB deduped)wandb: - 0.24MB of 0.89MB uploaded (0.00MB deduped)wandb: \ 0.38MB of 0.89MB uploaded (0.00MB deduped)wandb: | 0.81MB of 0.89MB uploaded (0.00MB deduped)wandb: / 0.81MB of 0.89MB uploaded (0.00MB deduped)wandb: - 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: \ 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: | 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: / 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: - 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: \ 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: | 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb: / 0.89MB of 0.89MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Run history:
wandb:    train/loss █▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      train/lr ▁▃▆███▇▇▇▇▆▅▅▄▄▃▃▃▂▂▁▁▁▁▁
wandb:   val/overall ▁▃▅▆▇█▇███
wandb: 
wandb: Run summary:
wandb:    train/loss 2.35396
wandb:      train/lr 0.0
wandb:   val/overall 36.12
wandb: 
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced anetqa_train_qa_full: https://wandb.ai/gengyuanzhang/sb_qa_anet/runs/yodr17fq
wandb: Find logs at: ./wandb/run-20231018_130833-yodr17fq/logs/debug.log
wandb: 
Loading /nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json:   0%|          | 0/32050 [00:00<?, ?it/s]Loading /nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json:  82%|████████▏ | 26307/32050 [00:00<00:00, 263053.32it/s]Loading /nfs/data2/zhang/AnetQA/qa_train/qa_train_light.json: 100%|██████████| 32050/32050 [00:00<00:00, 244914.49it/s]
Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json:   0%|          | 0/4695 [00:00<?, ?it/s]Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json: 100%|██████████| 4695/4695 [00:00<00:00, 218700.80it/s]
Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json:   0%|          | 0/4695 [00:00<?, ?it/s]Loading /nfs/data2/zhang/AnetQA/qa_val/qa_val_light.json: 100%|██████████| 4695/4695 [00:00<00:00, 223958.89it/s]
