rdzv_endpoint: worker-4:22225
[32m2023-10-18T23:57:50 | loopitr: [0mLogging to: /home/wiss/zhang/Jinhe/singularity/test_model/model_test_ret/train.log
[32m2023-10-18T23:57:50 | __main__: [0mconfig: 
{'data_root': '/home/wiss/zhang/nfs/Anet_sing', 'anno_root_downstream': '/home/wiss/zhang/Jinhe/singularity/Data/anetqa', 'train_type': 'anet_ret_train_1.json', 'train_file': ['${anno_root_downstream}/${train_type}', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'test_types': ['temporal_contact_swap'], 'test_file': {'temporal_contact_swap': ['${anno_root_downstream}/anet_ret_temporal_contact_swap.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'temporal_contact_swap_mani': ['${anno_root_downstream}/anet_ret_temporal_contact_swap_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'temporal_action_swap': ['${anno_root_downstream}/anet_ret_temporal_action_swap.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'temporal_action_swap_mani': ['${anno_root_downstream}/anet_ret_temporal_action_swap_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_same_entity': ['${anno_root_downstream}/anet_ret_neighborhood_same_entity.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_same_entity_mani': ['${anno_root_downstream}/anet_ret_neighborhood_same_entity_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_diff_entity': ['${anno_root_downstream}/anet_ret_neighborhood_diff_entity.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'neighborhood_diff_entity_mani': ['${anno_root_downstream}/anet_ret_neighborhood_diff_entity_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_spatial': ['${anno_root_downstream}/anet_ret_counter_spatial.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_spatial_mani': ['${anno_root_downstream}/anet_ret_counter_spatial_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_contact': ['${anno_root_downstream}/anet_ret_counter_contact.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_contact_mani': ['${anno_root_downstream}/anet_ret_counter_contact_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_action': ['${anno_root_downstream}/anet_ret_counter_action.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_action_mani': ['${anno_root_downstream}/anet_ret_counter_action_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_attribute': ['${anno_root_downstream}/anet_ret_counter_attribute.json', '/home/wiss/zhang/nfs/Anet_sing', 'video'], 'counter_attribute_mani': ['${anno_root_downstream}/anet_ret_counter_attribute_mani.json', '/home/wiss/zhang/nfs/Anet_sing', 'video']}, 'stop_key': 'val1/', 'is_paragraph_retrieval': True, 'text_encoder': 'bert-base-uncased', 'bert_config': 'configs/config_bert.json', 'vit_type': 'beit', 'vit_zoo': {'beit': 'microsoft/beit-base-patch16-224-pt22k-ft22k'}, 'vit_name_or_pretrained_path': '${vit_zoo[${vit_type}]}', 'temporal_vision_encoder': {'enable': True, 'num_layers': 2, 'update_pooler_embed': False}, 'add_temporal_embed': True, 'image_res': 224, 'embed_dim': 256, 'video_input': {'num_frames': 1, 'reader': 'decord', 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle'}, 'max_txt_l': 60, 'batch_size': {'image': 160, 'video': 32}, 'batch_size_test': {'image': 128, 'video': 32}, 'k_test': 128, 'temp': 0.01, 'loss_weight': {'itc': 1.0, 'itm': 1.0}, 'itm_hard_neg': True, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 30, 'min_lr_multi': 0.1, 'warmup_epochs': 0}, 'output_dir': '/home/wiss/zhang/Jinhe/singularity/test_model/model_test_ret', 'resume': False, 'pretrained_path': '/home/wiss/zhang/Jinhe/singularity/paper_results/ret_moviegraph/moviegraph_moviegraph_train_1_Seed42/ckpt_best.pth', 'evaluate': True, 'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'eval_offload': True, 'device': 'cuda', 'seed': 42, 'log_freq': 100, 'dist_url': 'env://', 'distributed': True, 'fp16': True, 'debug': False, 'num_workers': 24, 'wandb': {'enable': False, 'entity': 'gengyuanzhang', 'project': 'anet_ret'}, 'save_path': '/home/wiss/zhang/nfs/video_prober/singularity/anetqa/', '22225': None, 'rank': 0, 'world_size': 1, 'gpu': 0, 'dist_backend': 'nccl'}
[32m2023-10-18T23:57:50 | __main__: [0mtrain_file: ['${anno_root_downstream}/${train_type}', '/home/wiss/zhang/nfs/Anet_sing', 'video']
[32m2023-10-18T23:57:50 | tasks.pretrain: [0mCreating dataset for ret
[5m[31mWARNING[0m [32m2023-10-18T23:57:51 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  builtin_warn(*args, **kwargs)

[5m[31mWARNING[0m [32m2023-10-18T23:57:51 | py.warnings: [0m/home/wiss/zhang/Jinhe/singularity/utils/distributed.py:18: UserWarning: This DataLoader will create 24 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  builtin_warn(*args, **kwargs)

[32m2023-10-18T23:57:51 | tasks.shared_utils: [0mCreating model
[32m2023-10-18T23:57:55 | models.model_retrieval_base: [0mLoading vit pre-trained weights from huggingface microsoft/beit-base-patch16-224-pt22k-ft22k.
[5m[31mWARNING[0m [32m2023-10-18T23:57:58 | py.warnings: [0m/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[5m[31mWARNING[0m [32m2023-10-18T23:57:58 | py.warnings: [0m/home/wiss/zhang/anaconda3/envs/probe-sl/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1639180594101/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]

[32m2023-10-18T23:57:59 | models.model_retrieval_base: [0mInit new model with new image size 224, and load weights.
[32m2023-10-18T23:58:01 | models.model_retrieval_base: [0m_IncompatibleKeys(missing_keys=['encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'encoder.layer.11.attention.attention.relative_position_bias.relative_position_index'], unexpected_keys=[])
[32m2023-10-18T23:58:01 | models.model_retrieval_base: [0mBuild text_encoder bert-base-uncased
[32m2023-10-18T23:58:03 | models.model_retrieval_base: [0mBuild text_encoder bert-base-uncased, done!
[32m2023-10-18T23:58:03 | models.model_retrieval_base: [0mBuild temporal_vision_encoder (#layer=2), randomly initialised.
[32m2023-10-18T23:58:03 | models.model_retrieval_base: [0mBuild temporal_vision_encoder, done!
[32m2023-10-18T23:58:04 | utils.optimizer: [0moptimizer -- lr=1e-05 wd=0.02 len(p)=190
[32m2023-10-18T23:58:04 | utils.optimizer: [0moptimizer -- lr=1e-05 wd=0 len(p)=300
[32m2023-10-18T23:58:04 | tasks.shared_utils: [0mLoading checkpoint from /home/wiss/zhang/Jinhe/singularity/paper_results/ret_moviegraph/moviegraph_moviegraph_train_1_Seed42/ckpt_best.pth
state_dict.keys():  odict_keys(['temp', 'temporal_embeddings', 'vision_encoder.embeddings.cls_token', 'vision_encoder.embeddings.patch_embeddings.projection.weight', 'vision_encoder.embeddings.patch_embeddings.projection.bias', 'vision_encoder.encoder.layer.0.lambda_1', 'vision_encoder.encoder.layer.0.lambda_2', 'vision_encoder.encoder.layer.0.attention.attention.query.weight', 'vision_encoder.encoder.layer.0.attention.attention.query.bias', 'vision_encoder.encoder.layer.0.attention.attention.key.weight', 'vision_encoder.encoder.layer.0.attention.attention.value.weight', 'vision_encoder.encoder.layer.0.attention.attention.value.bias', 'vision_encoder.encoder.layer.0.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.0.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.0.attention.output.dense.weight', 'vision_encoder.encoder.layer.0.attention.output.dense.bias', 'vision_encoder.encoder.layer.0.intermediate.dense.weight', 'vision_encoder.encoder.layer.0.intermediate.dense.bias', 'vision_encoder.encoder.layer.0.output.dense.weight', 'vision_encoder.encoder.layer.0.output.dense.bias', 'vision_encoder.encoder.layer.0.layernorm_before.weight', 'vision_encoder.encoder.layer.0.layernorm_before.bias', 'vision_encoder.encoder.layer.0.layernorm_after.weight', 'vision_encoder.encoder.layer.0.layernorm_after.bias', 'vision_encoder.encoder.layer.1.lambda_1', 'vision_encoder.encoder.layer.1.lambda_2', 'vision_encoder.encoder.layer.1.attention.attention.query.weight', 'vision_encoder.encoder.layer.1.attention.attention.query.bias', 'vision_encoder.encoder.layer.1.attention.attention.key.weight', 'vision_encoder.encoder.layer.1.attention.attention.value.weight', 'vision_encoder.encoder.layer.1.attention.attention.value.bias', 'vision_encoder.encoder.layer.1.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.1.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.1.attention.output.dense.weight', 'vision_encoder.encoder.layer.1.attention.output.dense.bias', 'vision_encoder.encoder.layer.1.intermediate.dense.weight', 'vision_encoder.encoder.layer.1.intermediate.dense.bias', 'vision_encoder.encoder.layer.1.output.dense.weight', 'vision_encoder.encoder.layer.1.output.dense.bias', 'vision_encoder.encoder.layer.1.layernorm_before.weight', 'vision_encoder.encoder.layer.1.layernorm_before.bias', 'vision_encoder.encoder.layer.1.layernorm_after.weight', 'vision_encoder.encoder.layer.1.layernorm_after.bias', 'vision_encoder.encoder.layer.2.lambda_1', 'vision_encoder.encoder.layer.2.lambda_2', 'vision_encoder.encoder.layer.2.attention.attention.query.weight', 'vision_encoder.encoder.layer.2.attention.attention.query.bias', 'vision_encoder.encoder.layer.2.attention.attention.key.weight', 'vision_encoder.encoder.layer.2.attention.attention.value.weight', 'vision_encoder.encoder.layer.2.attention.attention.value.bias', 'vision_encoder.encoder.layer.2.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.2.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.2.attention.output.dense.weight', 'vision_encoder.encoder.layer.2.attention.output.dense.bias', 'vision_encoder.encoder.layer.2.intermediate.dense.weight', 'vision_encoder.encoder.layer.2.intermediate.dense.bias', 'vision_encoder.encoder.layer.2.output.dense.weight', 'vision_encoder.encoder.layer.2.output.dense.bias', 'vision_encoder.encoder.layer.2.layernorm_before.weight', 'vision_encoder.encoder.layer.2.layernorm_before.bias', 'vision_encoder.encoder.layer.2.layernorm_after.weight', 'vision_encoder.encoder.layer.2.layernorm_after.bias', 'vision_encoder.encoder.layer.3.lambda_1', 'vision_encoder.encoder.layer.3.lambda_2', 'vision_encoder.encoder.layer.3.attention.attention.query.weight', 'vision_encoder.encoder.layer.3.attention.attention.query.bias', 'vision_encoder.encoder.layer.3.attention.attention.key.weight', 'vision_encoder.encoder.layer.3.attention.attention.value.weight', 'vision_encoder.encoder.layer.3.attention.attention.value.bias', 'vision_encoder.encoder.layer.3.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.3.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.3.attention.output.dense.weight', 'vision_encoder.encoder.layer.3.attention.output.dense.bias', 'vision_encoder.encoder.layer.3.intermediate.dense.weight', 'vision_encoder.encoder.layer.3.intermediate.dense.bias', 'vision_encoder.encoder.layer.3.output.dense.weight', 'vision_encoder.encoder.layer.3.output.dense.bias', 'vision_encoder.encoder.layer.3.layernorm_before.weight', 'vision_encoder.encoder.layer.3.layernorm_before.bias', 'vision_encoder.encoder.layer.3.layernorm_after.weight', 'vision_encoder.encoder.layer.3.layernorm_after.bias', 'vision_encoder.encoder.layer.4.lambda_1', 'vision_encoder.encoder.layer.4.lambda_2', 'vision_encoder.encoder.layer.4.attention.attention.query.weight', 'vision_encoder.encoder.layer.4.attention.attention.query.bias', 'vision_encoder.encoder.layer.4.attention.attention.key.weight', 'vision_encoder.encoder.layer.4.attention.attention.value.weight', 'vision_encoder.encoder.layer.4.attention.attention.value.bias', 'vision_encoder.encoder.layer.4.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.4.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.4.attention.output.dense.weight', 'vision_encoder.encoder.layer.4.attention.output.dense.bias', 'vision_encoder.encoder.layer.4.intermediate.dense.weight', 'vision_encoder.encoder.layer.4.intermediate.dense.bias', 'vision_encoder.encoder.layer.4.output.dense.weight', 'vision_encoder.encoder.layer.4.output.dense.bias', 'vision_encoder.encoder.layer.4.layernorm_before.weight', 'vision_encoder.encoder.layer.4.layernorm_before.bias', 'vision_encoder.encoder.layer.4.layernorm_after.weight', 'vision_encoder.encoder.layer.4.layernorm_after.bias', 'vision_encoder.encoder.layer.5.lambda_1', 'vision_encoder.encoder.layer.5.lambda_2', 'vision_encoder.encoder.layer.5.attention.attention.query.weight', 'vision_encoder.encoder.layer.5.attention.attention.query.bias', 'vision_encoder.encoder.layer.5.attention.attention.key.weight', 'vision_encoder.encoder.layer.5.attention.attention.value.weight', 'vision_encoder.encoder.layer.5.attention.attention.value.bias', 'vision_encoder.encoder.layer.5.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.5.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.5.attention.output.dense.weight', 'vision_encoder.encoder.layer.5.attention.output.dense.bias', 'vision_encoder.encoder.layer.5.intermediate.dense.weight', 'vision_encoder.encoder.layer.5.intermediate.dense.bias', 'vision_encoder.encoder.layer.5.output.dense.weight', 'vision_encoder.encoder.layer.5.output.dense.bias', 'vision_encoder.encoder.layer.5.layernorm_before.weight', 'vision_encoder.encoder.layer.5.layernorm_before.bias', 'vision_encoder.encoder.layer.5.layernorm_after.weight', 'vision_encoder.encoder.layer.5.layernorm_after.bias', 'vision_encoder.encoder.layer.6.lambda_1', 'vision_encoder.encoder.layer.6.lambda_2', 'vision_encoder.encoder.layer.6.attention.attention.query.weight', 'vision_encoder.encoder.layer.6.attention.attention.query.bias', 'vision_encoder.encoder.layer.6.attention.attention.key.weight', 'vision_encoder.encoder.layer.6.attention.attention.value.weight', 'vision_encoder.encoder.layer.6.attention.attention.value.bias', 'vision_encoder.encoder.layer.6.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.6.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.6.attention.output.dense.weight', 'vision_encoder.encoder.layer.6.attention.output.dense.bias', 'vision_encoder.encoder.layer.6.intermediate.dense.weight', 'vision_encoder.encoder.layer.6.intermediate.dense.bias', 'vision_encoder.encoder.layer.6.output.dense.weight', 'vision_encoder.encoder.layer.6.output.dense.bias', 'vision_encoder.encoder.layer.6.layernorm_before.weight', 'vision_encoder.encoder.layer.6.layernorm_before.bias', 'vision_encoder.encoder.layer.6.layernorm_after.weight', 'vision_encoder.encoder.layer.6.layernorm_after.bias', 'vision_encoder.encoder.layer.7.lambda_1', 'vision_encoder.encoder.layer.7.lambda_2', 'vision_encoder.encoder.layer.7.attention.attention.query.weight', 'vision_encoder.encoder.layer.7.attention.attention.query.bias', 'vision_encoder.encoder.layer.7.attention.attention.key.weight', 'vision_encoder.encoder.layer.7.attention.attention.value.weight', 'vision_encoder.encoder.layer.7.attention.attention.value.bias', 'vision_encoder.encoder.layer.7.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.7.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.7.attention.output.dense.weight', 'vision_encoder.encoder.layer.7.attention.output.dense.bias', 'vision_encoder.encoder.layer.7.intermediate.dense.weight', 'vision_encoder.encoder.layer.7.intermediate.dense.bias', 'vision_encoder.encoder.layer.7.output.dense.weight', 'vision_encoder.encoder.layer.7.output.dense.bias', 'vision_encoder.encoder.layer.7.layernorm_before.weight', 'vision_encoder.encoder.layer.7.layernorm_before.bias', 'vision_encoder.encoder.layer.7.layernorm_after.weight', 'vision_encoder.encoder.layer.7.layernorm_after.bias', 'vision_encoder.encoder.layer.8.lambda_1', 'vision_encoder.encoder.layer.8.lambda_2', 'vision_encoder.encoder.layer.8.attention.attention.query.weight', 'vision_encoder.encoder.layer.8.attention.attention.query.bias', 'vision_encoder.encoder.layer.8.attention.attention.key.weight', 'vision_encoder.encoder.layer.8.attention.attention.value.weight', 'vision_encoder.encoder.layer.8.attention.attention.value.bias', 'vision_encoder.encoder.layer.8.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.8.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.8.attention.output.dense.weight', 'vision_encoder.encoder.layer.8.attention.output.dense.bias', 'vision_encoder.encoder.layer.8.intermediate.dense.weight', 'vision_encoder.encoder.layer.8.intermediate.dense.bias', 'vision_encoder.encoder.layer.8.output.dense.weight', 'vision_encoder.encoder.layer.8.output.dense.bias', 'vision_encoder.encoder.layer.8.layernorm_before.weight', 'vision_encoder.encoder.layer.8.layernorm_before.bias', 'vision_encoder.encoder.layer.8.layernorm_after.weight', 'vision_encoder.encoder.layer.8.layernorm_after.bias', 'vision_encoder.encoder.layer.9.lambda_1', 'vision_encoder.encoder.layer.9.lambda_2', 'vision_encoder.encoder.layer.9.attention.attention.query.weight', 'vision_encoder.encoder.layer.9.attention.attention.query.bias', 'vision_encoder.encoder.layer.9.attention.attention.key.weight', 'vision_encoder.encoder.layer.9.attention.attention.value.weight', 'vision_encoder.encoder.layer.9.attention.attention.value.bias', 'vision_encoder.encoder.layer.9.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.9.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.9.attention.output.dense.weight', 'vision_encoder.encoder.layer.9.attention.output.dense.bias', 'vision_encoder.encoder.layer.9.intermediate.dense.weight', 'vision_encoder.encoder.layer.9.intermediate.dense.bias', 'vision_encoder.encoder.layer.9.output.dense.weight', 'vision_encoder.encoder.layer.9.output.dense.bias', 'vision_encoder.encoder.layer.9.layernorm_before.weight', 'vision_encoder.encoder.layer.9.layernorm_before.bias', 'vision_encoder.encoder.layer.9.layernorm_after.weight', 'vision_encoder.encoder.layer.9.layernorm_after.bias', 'vision_encoder.encoder.layer.10.lambda_1', 'vision_encoder.encoder.layer.10.lambda_2', 'vision_encoder.encoder.layer.10.attention.attention.query.weight', 'vision_encoder.encoder.layer.10.attention.attention.query.bias', 'vision_encoder.encoder.layer.10.attention.attention.key.weight', 'vision_encoder.encoder.layer.10.attention.attention.value.weight', 'vision_encoder.encoder.layer.10.attention.attention.value.bias', 'vision_encoder.encoder.layer.10.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.10.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.10.attention.output.dense.weight', 'vision_encoder.encoder.layer.10.attention.output.dense.bias', 'vision_encoder.encoder.layer.10.intermediate.dense.weight', 'vision_encoder.encoder.layer.10.intermediate.dense.bias', 'vision_encoder.encoder.layer.10.output.dense.weight', 'vision_encoder.encoder.layer.10.output.dense.bias', 'vision_encoder.encoder.layer.10.layernorm_before.weight', 'vision_encoder.encoder.layer.10.layernorm_before.bias', 'vision_encoder.encoder.layer.10.layernorm_after.weight', 'vision_encoder.encoder.layer.10.layernorm_after.bias', 'vision_encoder.encoder.layer.11.lambda_1', 'vision_encoder.encoder.layer.11.lambda_2', 'vision_encoder.encoder.layer.11.attention.attention.query.weight', 'vision_encoder.encoder.layer.11.attention.attention.query.bias', 'vision_encoder.encoder.layer.11.attention.attention.key.weight', 'vision_encoder.encoder.layer.11.attention.attention.value.weight', 'vision_encoder.encoder.layer.11.attention.attention.value.bias', 'vision_encoder.encoder.layer.11.attention.attention.relative_position_bias.relative_position_bias_table', 'vision_encoder.encoder.layer.11.attention.attention.relative_position_bias.relative_position_index', 'vision_encoder.encoder.layer.11.attention.output.dense.weight', 'vision_encoder.encoder.layer.11.attention.output.dense.bias', 'vision_encoder.encoder.layer.11.intermediate.dense.weight', 'vision_encoder.encoder.layer.11.intermediate.dense.bias', 'vision_encoder.encoder.layer.11.output.dense.weight', 'vision_encoder.encoder.layer.11.output.dense.bias', 'vision_encoder.encoder.layer.11.layernorm_before.weight', 'vision_encoder.encoder.layer.11.layernorm_before.bias', 'vision_encoder.encoder.layer.11.layernorm_after.weight', 'vision_encoder.encoder.layer.11.layernorm_after.bias', 'vision_encoder.pooler.layernorm.weight', 'vision_encoder.pooler.layernorm.bias', 'vision_layernorm.weight', 'vision_layernorm.bias', 'text_encoder.embeddings.position_ids', 'text_encoder.embeddings.word_embeddings.weight', 'text_encoder.embeddings.position_embeddings.weight', 'text_encoder.embeddings.token_type_embeddings.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'temporal_vision_encoder.layer.0.attention.self.query.weight', 'temporal_vision_encoder.layer.0.attention.self.query.bias', 'temporal_vision_encoder.layer.0.attention.self.key.weight', 'temporal_vision_encoder.layer.0.attention.self.key.bias', 'temporal_vision_encoder.layer.0.attention.self.value.weight', 'temporal_vision_encoder.layer.0.attention.self.value.bias', 'temporal_vision_encoder.layer.0.attention.output.dense.weight', 'temporal_vision_encoder.layer.0.attention.output.dense.bias', 'temporal_vision_encoder.layer.0.attention.output.LayerNorm.weight', 'temporal_vision_encoder.layer.0.attention.output.LayerNorm.bias', 'temporal_vision_encoder.layer.0.intermediate.dense.weight', 'temporal_vision_encoder.layer.0.intermediate.dense.bias', 'temporal_vision_encoder.layer.0.output.dense.weight', 'temporal_vision_encoder.layer.0.output.dense.bias', 'temporal_vision_encoder.layer.0.output.LayerNorm.weight', 'temporal_vision_encoder.layer.0.output.LayerNorm.bias', 'temporal_vision_encoder.layer.1.attention.self.query.weight', 'temporal_vision_encoder.layer.1.attention.self.query.bias', 'temporal_vision_encoder.layer.1.attention.self.key.weight', 'temporal_vision_encoder.layer.1.attention.self.key.bias', 'temporal_vision_encoder.layer.1.attention.self.value.weight', 'temporal_vision_encoder.layer.1.attention.self.value.bias', 'temporal_vision_encoder.layer.1.attention.output.dense.weight', 'temporal_vision_encoder.layer.1.attention.output.dense.bias', 'temporal_vision_encoder.layer.1.attention.output.LayerNorm.weight', 'temporal_vision_encoder.layer.1.attention.output.LayerNorm.bias', 'temporal_vision_encoder.layer.1.intermediate.dense.weight', 'temporal_vision_encoder.layer.1.intermediate.dense.bias', 'temporal_vision_encoder.layer.1.output.dense.weight', 'temporal_vision_encoder.layer.1.output.dense.bias', 'temporal_vision_encoder.layer.1.output.LayerNorm.weight', 'temporal_vision_encoder.layer.1.output.LayerNorm.bias'])
[32m2023-10-18T23:58:14 | models.utils: [0mLoad temporal_embeddings, lengths: 4-->1
[32m2023-10-18T23:58:14 | tasks.shared_utils: [0m<All keys matched successfully>
[32m2023-10-18T23:58:14 | tasks.shared_utils: [0mLoaded checkpoint from /home/wiss/zhang/Jinhe/singularity/paper_results/ret_moviegraph/moviegraph_moviegraph_train_1_Seed42/ckpt_best.pth
[32m2023-10-18T23:58:14 | __main__: [0mStart evaluation
[32m2023-10-18T23:58:14 | tasks.retrieval_utils: [0mStart evaluation for media_type=video
[32m2023-10-18T23:58:14 | tasks.retrieval_utils: [0mComputing dual encoder features...
